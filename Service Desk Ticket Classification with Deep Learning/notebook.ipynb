{"cells":[{"source":"![servicedesk](servicedesk.png)\n\nCleverSupport is a company at the forefront of AI innovation, specializing in the development of AI-driven solutions to enhance customer support services. Their latest endeavor is to engineer a text classification system that can automatically categorize customer complaints. \n\nYour role as a data scientist involves the creation of a sophisticated machine learning model that can accurately assign complaints to specific categories, such as mortgage, credit card, money transfers, debt collection, etc.","metadata":{"executionCancelledAt":null,"executionTime":165,"lastExecutedAt":1707667023665,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"CleverSupport is a company at the forefront of AI innovation, specializing in the development of AI-driven solutions to enhance customer support services. Their latest endeavor is to engineer a text classification system that can autonomously categorize customer complaints. \n\nYour role as a data scientist involves the creation of a sophisticated machine learning model that can accurately assign complaints to specific categories, such as technical issues, billing inquiries, cancellation requests, refunds, and product information requests."},"id":"e5870ae0-6165-459e-9c40-0f282883be7b","cell_type":"markdown"},{"source":"# Project Instructions\nClassify service desk tickets into categories using a CNN to streamline customer services.\n\n- Define a CNN classifier with the following layers: an embedding layer, a 1D convolution layer, and a linear layer.\n- Train your classifier on `train_data` using a suitable optimizer. Run your training for only 3 epochs.\n- Test your classifier on `test_data`, storing your predictions in a list called `predictions`.\n- Calculate the accuracy, per-class precision, and recall for your trained classifier on the `test_data`. Save the metrics as variables with the corresponding names: `accuracy`, `precision`, and `recall`, with precision and recall saved as lists.","metadata":{},"cell_type":"markdown","id":"fb59635b-b4bf-4bf7-829a-da251a53abf5"},{"source":"# How to approach the project\n1. Defining the classifier\n2. Training the classifier\n3. Testing the classifier\n\n## Steps to complete\n\n### 1. Defining the classifier\nDefine a class containing all the appropriate layers, and a method to perform the forward pass over a batch of input text.\n\n#### Creating a class to contain the layers of the classifier\n- Define a class called `TicketClassifier` that inherits from PyTorch's `nn.Module` class.\n\n#### Adding an embedding layer\n- Use PyTorch's `nn.Embedding` class to define the embedding layer.\n- Create an instance of it in the `TicketClassifier` class's constructor and assign it to an instance variable such as `self.embedding`.\n\n#### Adding a convolution ayer\n- Use PyTorch's `nn.Conv1d` class to define the 1D convolution layer.\n- Create an instance of it in the `TicketClassifier` class's constructor and assign it to an instance variable such as `self.conv`.\n\n#### Adding a linear layer\n- Use PyTorch's `nn.Linear` class to define the linear layer.\n- Create an instance of it in the `TicketClassifier` class's constructor and assign it to an instance variable such as `self.fc`.\n\n#### Define a .forward() method\n- Finally, define a `.forward()` method that passes the input through the embedding and convolution layer, applies `nn.functional.relu` on the output, and finally applies linear layer before returning the output.\n\n### 2. Training the classifier\nDefine a training loop that loops over the dataset, calculating the loss and propagating it backwards through the network.\n\n#### Define a suitable loss criterion\n- Use PyTorch's `nn.CrossEntropyLoss`, since this is a multi-class classification problem.\n\n#### Define an optimizer\n- Use PyTorch's `optim.Adam` optimizer.\n\n### 3. Testing the classifier\nUse your trained model to classify the text in the test set, and calculate the appropriate metrics.\n\n#### Predict the category of each ticket in the test data.\n- Invoke `model()` on your input data to pass the data through the network.\n- Use `torch.argmax()` to find the category with the highest predicted probability.\n\n#### Calculate the accuracy\n- Use `torchmetrics.Accuracy` to calculate the accuracy.\n\n#### Calculate the precision and recall\n- Use `torchmetrics.Precision` and `torchmetrics.Recall` to calculate the precision and recall.","metadata":{},"cell_type":"markdown","id":"b8d2967a-14de-47c5-8ae6-0f911c936f64"},{"source":"!pip install torchmetrics","metadata":{"executionCancelledAt":null,"executionTime":3066,"lastExecutedAt":1725258900817,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"!pip install torchmetrics","outputsMetadata":{"0":{"height":384,"type":"stream"}},"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"lastExecutedByKernel":"baaa197d-df62-4561-b455-e7b73d35788e"},"id":"0dd4beb4-2329-4b0d-8a34-2354ee9c7fb4","cell_type":"code","execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":"Defaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: torchmetrics in /home/repl/.local/lib/python3.8/site-packages (1.4.1)\nRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.23.2)\nRequirement already satisfied: packaging>17.1 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (23.2)\nRequirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.13.0)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /home/repl/.local/lib/python3.8/site-packages (from torchmetrics) (0.11.6)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (4.9.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (65.6.3)\nRequirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->torchmetrics) (11.7.99)\nRequirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->torchmetrics) (8.5.0.96)\nRequirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->torchmetrics) (11.10.3.66)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->torchmetrics) (11.7.99)\nRequirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->torchmetrics) (0.38.4)\n"}]},{"source":"from collections import Counter\nimport nltk, json\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torchmetrics import Accuracy, Precision, Recall","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"executionCancelledAt":null,"executionTime":58,"lastExecutedAt":1725258900878,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from collections import Counter\nimport nltk, json\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torchmetrics import Accuracy, Precision, Recall","lastExecutedByKernel":"baaa197d-df62-4561-b455-e7b73d35788e"},"id":"2fa90b61-0244-4236-aa93-e33a7a088eec","cell_type":"code","execution_count":33,"outputs":[]},{"source":"nltk.download('punkt')","metadata":{"executionCancelledAt":null,"executionTime":55,"lastExecutedAt":1725258900933,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"nltk.download('punkt')","outputsMetadata":{"0":{"height":59,"type":"stream"}},"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"lastExecutedByKernel":"baaa197d-df62-4561-b455-e7b73d35788e"},"id":"37a51a81-1301-4a80-b8c6-716faaff4c5c","cell_type":"code","execution_count":34,"outputs":[{"output_type":"stream","name":"stderr","text":"[nltk_data] Downloading package punkt to /home/repl/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"},{"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{},"execution_count":34}]},{"source":"# Import data and labels\nwith open(\"words.json\", 'r') as f1:\n    words = json.load(f1)\nwith open(\"text.json\", 'r') as f2:\n    text = json.load(f2)\nlabels = np.load('labels.npy')","metadata":{"executionCancelledAt":null,"executionTime":127,"lastExecutedAt":1725258901060,"lastExecutedByKernel":"baaa197d-df62-4561-b455-e7b73d35788e","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import data and labels\nwith open(\"words.json\", 'r') as f1:\n    words = json.load(f1)\nwith open(\"text.json\", 'r') as f2:\n    text = json.load(f2)\nlabels = np.load('labels.npy')"},"id":"e1b12eaf-e55c-422c-94a2-b0197c465a1b","cell_type":"code","execution_count":35,"outputs":[]},{"source":"# Print the contents of the data\nprint(\"Words:\")\nprint(words)  # prints the entire dictionary loaded from words.json\n\nprint(\"\\nText:\")\nprint(text)  # prints the entire dictionary loaded from text.json\n\nprint(\"\\nLabels:\")\nprint(labels)  # prints the entire array loaded from labels.npy","metadata":{"executionCancelledAt":null,"executionTime":324,"lastExecutedAt":1725258901386,"lastExecutedByKernel":"baaa197d-df62-4561-b455-e7b73d35788e","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Print the contents of the data\nprint(\"Words:\")\nprint(words)  # prints the entire dictionary loaded from words.json\n\nprint(\"\\nText:\")\nprint(text)  # prints the entire dictionary loaded from text.json\n\nprint(\"\\nLabels:\")\nprint(labels)  # prints the entire array loaded from labels.npy"},"cell_type":"code","id":"e633ac28-92ba-4adc-a560-0d217754a4af","outputs":[{"output_type":"stream","text":"IOPub data rate exceeded.\nThe Jupyter server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--ServerApp.iopub_data_rate_limit`.\n\nCurrent values:\nServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\nServerApp.rate_limit_window=3.0 (secs)\n\n","name":"stderr"}],"execution_count":36},{"source":"# Dictionaries to store the word to index mappings and vice versa\nword2idx = {o:i for i,o in enumerate(words)}\nidx2word = {i:o for i,o in enumerate(words)}","metadata":{"executionCancelledAt":null,"executionTime":54,"lastExecutedAt":1725258901440,"lastExecutedByKernel":"baaa197d-df62-4561-b455-e7b73d35788e","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Dictionaries to store the word to index mappings and vice versa\nword2idx = {o:i for i,o in enumerate(words)}\nidx2word = {i:o for i,o in enumerate(words)}"},"cell_type":"code","id":"765fc0ba-af56-412d-a2ee-a828f88ee676","outputs":[],"execution_count":37},{"source":"# Looking up the mapping dictionary and assigning the index to the respective words\nfor i, sentence in enumerate(text):\n    text[i] = [word2idx[word] if word in word2idx else 0 for word in sentence]","metadata":{"executionCancelledAt":null,"executionTime":182,"lastExecutedAt":1725258901623,"lastExecutedByKernel":"baaa197d-df62-4561-b455-e7b73d35788e","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Looking up the mapping dictionary and assigning the index to the respective words\nfor i, sentence in enumerate(text):\n    text[i] = [word2idx[word] if word in word2idx else 0 for word in sentence]"},"cell_type":"code","id":"a83a2ffd-f3c6-4e3e-8c26-7bed049e19e1","outputs":[],"execution_count":38},{"source":"# Defining a function that either shortens sentences or pads sentences with 0 to a fixed length\ndef pad_input(sentences, seq_len):\n    features = np.zeros((len(sentences), seq_len),dtype=int)\n    for ii, review in enumerate(sentences):\n        if len(review) != 0:\n            features[ii, -len(review):] = np.array(review)[:seq_len]\n    return features\n\ntext = pad_input(text, 50)","metadata":{"executionCancelledAt":null,"executionTime":86,"lastExecutedAt":1725258901711,"lastExecutedByKernel":"baaa197d-df62-4561-b455-e7b73d35788e","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Defining a function that either shortens sentences or pads sentences with 0 to a fixed length\ndef pad_input(sentences, seq_len):\n    features = np.zeros((len(sentences), seq_len),dtype=int)\n    for ii, review in enumerate(sentences):\n        if len(review) != 0:\n            features[ii, -len(review):] = np.array(review)[:seq_len]\n    return features\n\ntext = pad_input(text, 50)"},"cell_type":"code","id":"290c4321-4b88-4eee-a541-4f04ec5bac44","outputs":[],"execution_count":39},{"source":"# Splitting dataset\ntrain_text, test_text, train_label, test_label = train_test_split(text, labels, test_size=0.2, random_state=42)\n\ntrain_data = TensorDataset(torch.from_numpy(train_text), torch.from_numpy(train_label).long())\ntest_data = TensorDataset(torch.from_numpy(test_text), torch.from_numpy(test_label).long())","metadata":{"executionCancelledAt":null,"executionTime":55,"lastExecutedAt":1725258901768,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Splitting dataset\ntrain_text, test_text, train_label, test_label = train_test_split(text, labels, test_size=0.2, random_state=42)\n\ntrain_data = TensorDataset(torch.from_numpy(train_text), torch.from_numpy(train_label).long())\ntest_data = TensorDataset(torch.from_numpy(test_text), torch.from_numpy(test_label).long())","lastExecutedByKernel":"baaa197d-df62-4561-b455-e7b73d35788e"},"id":"f2654836-631f-415e-9922-5ab3bafaaafa","cell_type":"code","execution_count":40,"outputs":[]},{"source":"# Start coding here\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torchmetrics import Accuracy, Precision, Recall","metadata":{"executionCancelledAt":null,"executionTime":51,"lastExecutedAt":1725258901820,"lastExecutedByKernel":"baaa197d-df62-4561-b455-e7b73d35788e","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Start coding here\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torchmetrics import Accuracy, Precision, Recall"},"cell_type":"code","id":"a762509f-e8ac-4774-9e80-ba8283710122","outputs":[],"execution_count":41},{"source":"batch_size = 400\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\ntest_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1725258901868,"lastExecutedByKernel":"baaa197d-df62-4561-b455-e7b73d35788e","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"batch_size = 400\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\ntest_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)"},"cell_type":"code","id":"83f6b5a2-f127-4930-8bfd-284c8a60a4bb","outputs":[],"execution_count":42},{"source":"# Define the classifier class\nclass TicketClassifier(nn.Module):\n    def __init__(self, vocab_size, embed_dim, target_size):\n        super(TicketClassifier, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.conv = nn.Conv1d(embed_dim, embed_dim, kernel_size=3, stride=1, padding=1)\n        self.fc = nn.Linear(embed_dim, target_size)\n\n    def forward(self, text):\n        embedded = self.embedding(text).permute(0, 2, 1)\n        conved = F.relu(self.conv(embedded))\n        conved = conved.mean(dim=2) \n        return self.fc(conved)","metadata":{"executionCancelledAt":null,"executionTime":55,"lastExecutedAt":1725258901924,"lastExecutedByKernel":"baaa197d-df62-4561-b455-e7b73d35788e","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Define the classifier class\nclass TicketClassifier(nn.Module):\n    def __init__(self, vocab_size, embed_dim, target_size):\n        super(TicketClassifier, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.conv = nn.Conv1d(embed_dim, embed_dim, kernel_size=3, stride=1, padding=1)\n        self.fc = nn.Linear(embed_dim, target_size)\n\n    def forward(self, text):\n        embedded = self.embedding(text).permute(0, 2, 1)\n        conved = F.relu(self.conv(embedded))\n        conved = conved.mean(dim=2) \n        return self.fc(conved)"},"cell_type":"code","id":"1dca51ca-4797-4c0d-94e9-469d01e0602e","outputs":[],"execution_count":43},{"source":"vocab_size = len(word2idx) + 1\ntarget_size = len(np.unique(labels))\nembedding_dim = 64","metadata":{"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1725258901976,"lastExecutedByKernel":"baaa197d-df62-4561-b455-e7b73d35788e","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"vocab_size = len(word2idx) + 1\ntarget_size = len(np.unique(labels))\nembedding_dim = 64"},"cell_type":"code","id":"9060863a-c8c3-47c7-aa1a-79842ed1c831","outputs":[],"execution_count":44},{"source":"# Create an instance of the TicketClassifier class\nmodel = TicketClassifier(vocab_size, embedding_dim, target_size)\n\nlr = 0.05\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\nepochs = 3","metadata":{"executionCancelledAt":null,"executionTime":51,"lastExecutedAt":1725258902028,"lastExecutedByKernel":"baaa197d-df62-4561-b455-e7b73d35788e","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Create an instance of the TicketClassifier class\nmodel = TicketClassifier(vocab_size, embedding_dim, target_size)\n\nlr = 0.05\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\nepochs = 3"},"cell_type":"code","id":"07c5d860-ebdf-41b9-8d35-e17044399f20","outputs":[],"execution_count":45},{"source":"# Train the model\nmodel.train()\nfor i in range(epochs):\n    running_loss, num_processed = 0,0\n    for inputs, labels in train_loader:\n        model.zero_grad()\n        output = model(inputs)\n        loss = criterion(output, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n        num_processed += len(inputs)\n    print(f\"Epoch: {i+1}, Loss: {running_loss/num_processed}\")","metadata":{"executionCancelledAt":null,"executionTime":1771,"lastExecutedAt":1725258903800,"lastExecutedByKernel":"baaa197d-df62-4561-b455-e7b73d35788e","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Train the model\nmodel.train()\nfor i in range(epochs):\n    running_loss, num_processed = 0,0\n    for inputs, labels in train_loader:\n        model.zero_grad()\n        output = model(inputs)\n        loss = criterion(output, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n        num_processed += len(inputs)\n    print(f\"Epoch: {i+1}, Loss: {running_loss/num_processed}\")","outputsMetadata":{"0":{"height":59,"type":"stream"}}},"id":"d2b3c50c-66b1-40ed-a17c-038e7addc7ec","cell_type":"code","execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":"Epoch: 1, Loss: 0.0038546013832092287\nEpoch: 2, Loss: 0.0015976080670952798\nEpoch: 3, Loss: 0.0007425523810088634\n"}]},{"source":"accuracy_metric = Accuracy(task='multiclass', num_classes=5)\nprecision_metric = Precision(task='multiclass', num_classes=5, average=None)\nrecall_metric = Recall(task='multiclass', num_classes=5, average=None)","metadata":{"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1725258903852,"lastExecutedByKernel":"baaa197d-df62-4561-b455-e7b73d35788e","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"accuracy_metric = Accuracy(task='multiclass', num_classes=5)\nprecision_metric = Precision(task='multiclass', num_classes=5, average=None)\nrecall_metric = Recall(task='multiclass', num_classes=5, average=None)"},"cell_type":"code","id":"6e6e27a5-c0e1-4c8a-8d81-5bf95054ae30","outputs":[],"execution_count":47},{"source":"# Evaluate model on test set\nmodel.eval()\npredicted = []\n\nfor i, (inputs, labels) in enumerate(test_loader):\n    output = model(inputs)\n    cat = torch.argmax(output, dim=-1)\n    predicted.extend(cat.tolist())\n    accuracy_metric(cat, labels)\n    precision_metric(cat, labels)\n    recall_metric(cat, labels)\n\naccuracy = accuracy_metric.compute().item()\nprecision = precision_metric.compute().tolist()\nrecall = recall_metric.compute().tolist()\nprint('Accuracy:', accuracy)\nprint('Precision (per class):', precision)\nprint('Recall (per class):', recall)","metadata":{"executionCancelledAt":null,"executionTime":54,"lastExecutedAt":1725258903906,"lastExecutedByKernel":"baaa197d-df62-4561-b455-e7b73d35788e","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Evaluate model on test set\nmodel.eval()\npredicted = []\n\nfor i, (inputs, labels) in enumerate(test_loader):\n    output = model(inputs)\n    cat = torch.argmax(output, dim=-1)\n    predicted.extend(cat.tolist())\n    accuracy_metric(cat, labels)\n    precision_metric(cat, labels)\n    recall_metric(cat, labels)\n\naccuracy = accuracy_metric.compute().item()\nprecision = precision_metric.compute().tolist()\nrecall = recall_metric.compute().tolist()\nprint('Accuracy:', accuracy)\nprint('Precision (per class):', precision)\nprint('Recall (per class):', recall)","outputsMetadata":{"0":{"height":101,"type":"stream"}}},"cell_type":"code","id":"1c161702-110d-4db2-8b39-5c17ff22d531","outputs":[{"output_type":"stream","name":"stdout","text":"Accuracy: 0.7979999780654907\nPrecision (per class): [0.6682464480400085, 0.7168949842453003, 0.9345238208770752, 0.8118279576301575, 0.8888888955116272]\nRecall (per class): [0.734375, 0.8263157606124878, 0.7268518805503845, 0.7864583134651184, 0.9142857193946838]\n"}],"execution_count":48}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}