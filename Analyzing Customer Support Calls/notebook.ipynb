{"cells":[{"source":"<p align=\"center\" width=\"100%\">\n    <img width=\"40%\" src=\"customer_support_icon.JPG\"> \n</p>\n\nA retail company is on a transformative journey, aiming to elevate their customer services through cutting-edge advancements in Speech Recognition and Natural Language Processing (NLP). As the machine learning engineer for this initiative, you are tasked with developing functionalities that not only convert customer support audio calls into text but also explore methodologies to extract insights from transcribed texts.\n\nIn this dynamic project, we leverage the power of `SpeechRecognition`, `Pydub`, and `spaCy` – three open-source packages that form the backbone of your solution. Your objectives are:\n  - Transcribe a sample customer audio call, stored at `sample_customer_call.wav`, to showcase the power of open-source speech recognition technology.\n  - Analyze sentiment, identify common named entities, and enhance user experience by searching for the most similar customer calls based on a given query from a subset of their pre-transcribed call data, stored at `customer_call_transcriptions.csv`.\n\nThis project is an opportunity to unlock the potential of machine learning to revolutionize customer support. Let's delve into the interplay between technology and service excellence.","metadata":{},"id":"d5e81b43-ccfd-4fc6-902c-59cd49aa9913","cell_type":"markdown"},{"source":"# Project Instructions\n\nThe company hired you to help them analyze their customer support calls with the following tasks:\n\nIs the audio compatible for future speech recognition modeling?\n\n- Convert `sample_customer_call.wav` into text and store the result in `transcribed_text`.\n- Find the frame rate and number of channels of this audio and save your answer as two numeric variables: `frame_rate`, `number_channels`.\n\nHow many calls have a true positive sentiment?\n\n- Perform sentiment analysis on `customer_call_transcriptions.csv` and find the number of `true positive` predictions; save an integer value to `true_positive`.\n- Use the `compound` score in the `vader` module and threshold values of `0.05` and `-0.05` to set a sentiment to `positive`, `neutral` or `negative`.\n\nWhat is the most frequently named entity across all of the transcriptions?\n\n- Save your answer as a string variable `most_freq_ent`.\n\nWhich call is the most similar to \"wrong package delivery\"?\n\n- Save your answer as a string variable `most_similar_text`.","metadata":{},"cell_type":"markdown","id":"f0b6714c-f78f-4397-a2d8-8c8deeeda06a"},{"source":"# How to approach the project\n1. Implement speech recognition and calculate audio statistics\n2. Perform sentiment analysis\n3. Run named entity recognition\n4. Find most similar texts\n\n## Steps to complete\n\n### 1. Implement speech recognition and calculate audio statistics\nUse the .recognize_google() function from SpeechRegontion's Recognizer class to transcribe audio to text. Then, use AudioSegment module in Pydub library to convert the audio to a segment and extract statistics.\n\n#### Recognizer functions\n- First define a `.Recognizer()` object and convert the audio file using `AudioFile` and `record` functions to convert the audio file to the audio data object.\n- Use ``.recognize_google()` function on the audio data to convert the audio to text.\n\n#### Pydub functions\n- You can use ``.from_file()` function from `AudioSegment` module to create a audio segment object and save your results as numeric variables `channels` and `frame_rate`.\n\n### 2. Perform sentiment analysis\nLoad the `customer_call_transcriptions.csv` file and store the data in a `pandas` DataFrame. Then, define a `SentimentIntensityAnalyzer()` object that can be used to extract sentiment scores for a given text.\n\n#### Sentiment polarity scores\n- The `.polarity_scores()` function of `SentimentIntensityAnalyzer` class will return multiple scores. You can pass a text to this function and calculate a `scores` dictionary for a given input text.\n- In this project, you will use `compound` score.\n- Per each text, you can access the `compound_score` by using `scores[\"compound\"]`.\n- If the value of the `compound_score` has a value of greater than or equal to `0.05`, set the sentiment to `positive`. If the score is less than or equal to `-0.05`, set the sentiment to `negative` and otherwise set the sentiment to `neutral`.\n\n#### Run sentiment analysis\n- Store the results of the `.polarity_score()` function per each text in the associated row in a new `sentiment_predicted` column.\n- To calculate the numeric `true_positive variable`, find the number of rows in the `df` that `sentiment_label` is equal to `sentiment_predicted` and `sentiment_predicted` is equal to `positive`. You can use `.loc` to filter the DataFrame object.\n\n### 3. Run named entity recognition\nUse the small English Language `spaCy` model to extract all the named entities per each text from the DataFrame of the transcribed customer calls. For this project, you will only need to store the `text` attribute of a named entity. You are not required to clean or preprocess (e.g. lower case) the texts for this task.\n\n#### spaCy document object and NER\n- After loading a `spaCy` model as an `nlp` object, you can first create a `spaCy` document object (`doc`) by passing a given text to the `nlp` object.\n- Then you can iterate through the named entities of a text by accessing the ``.ents` attribute of the `doc` object.\n- You can access a named entity text by accessing `.text` attribute of an entity.\n\n#### Entity processing\n- You can store all the entities of all transcriptions in a list.\n- Next, you can identify the most frequent named entity text by finding the frequency of each entity text in this list.\n- Save your result as a string variable `most_freq_ent`.\n\n### 4. Find most similar texts\nIn order to identify the most similar customer calls to a given query such as `wrong package delivery`, you can use the `.similarity()` function of a `spaCy` `doc` object to calculate a `similarity` score between two `doc` objects. The most similar text to a given query will have a higher `similarity` score.\n\n#### Similarity scores\n- To calculate the similarity score between `doc1` and `doc2`, two `spaCy` document objects, you can use `doc1.similarity(doc2)`.\n- Higher similarity scores can help you identify the most similar texts to a given query.\n- Save your result in a string variable as `most_similar_text`.","metadata":{},"cell_type":"markdown","id":"2ec75b17-ed88-47d4-b35e-5ac4012c32ba"},{"source":"!pip install SpeechRecognition\n!pip install pydub\n!pip install spacy\n!python3 -m spacy download en_core_web_sm","metadata":{"executionCancelledAt":null,"executionTime":18694,"lastExecutedAt":1724827801725,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"!pip install SpeechRecognition\n!pip install pydub\n!pip install spacy\n!python3 -m spacy download en_core_web_sm","outputsMetadata":{"0":{"height":613,"type":"stream"}},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false},"lastExecutedByKernel":"af742785-50ad-4102-9c59-a43e0de9dc32"},"id":"d0f1598e-18a8-45d5-8387-bf2f5ce4ffd6","cell_type":"code","execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":"Defaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: SpeechRecognition in /home/repl/.local/lib/python3.8/site-packages (3.10.4)\nRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.8/dist-packages (from SpeechRecognition) (2.31.0)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from SpeechRecognition) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.26.0->SpeechRecognition) (2.0.12)\nRequirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.26.0->SpeechRecognition) (2.8)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.26.0->SpeechRecognition) (1.25.8)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.26.0->SpeechRecognition) (2019.11.28)\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: pydub in /home/repl/.local/lib/python3.8/site-packages (0.25.1)\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: spacy in /usr/local/lib/python3.8/dist-packages (3.6.1)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.9)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.7)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.8)\nRequirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.8/dist-packages (from spacy) (8.1.12)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.4.7)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.9)\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.9.0)\nRequirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.2)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (6.4.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (4.64.0)\nRequirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.23.2)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.31.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.10.12)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.1.2)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy) (65.6.3)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (23.2)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.3.0)\nRequirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\nRequirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.8)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.11.28)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.10)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.3)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy) (2.1.1)\nDefaulting to user installation because normal site-packages is not writeable\nCollecting en-core-web-sm==3.6.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m134.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.8/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.9)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.8)\nRequirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.7)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.9)\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\nRequirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.64.0)\nRequirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.2)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.12)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (65.6.3)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.2)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\nRequirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.12)\nRequirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.8)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.25.8)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2019.11.28)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.10)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.3)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.1)\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_sm')\n"}]},{"source":"# Start coding here\n# Before you start\n# In order to complete the project you may wish to install SpeechRecognition, Pydub and spaCy libraries and download pretrained spaCy small English Language model.\n\n# Import required libraries\nimport pandas as pd\n\nimport nltk\nnltk.download('vader_lexicon')\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nimport speech_recognition as sr\nfrom pydub import AudioSegment\n\nimport spacy","metadata":{"executionCancelledAt":null,"executionTime":49,"lastExecutedAt":1724827801776,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Start coding here\n# Before you start\n# In order to complete the project you may wish to install SpeechRecognition, Pydub and spaCy libraries and download pretrained spaCy small English Language model.\n\n# Import required libraries\nimport pandas as pd\n\nimport nltk\nnltk.download('vader_lexicon')\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nimport speech_recognition as sr\nfrom pydub import AudioSegment\n\nimport spacy","outputsMetadata":{"0":{"height":77,"type":"stream"}},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false},"lastExecutedByKernel":"af742785-50ad-4102-9c59-a43e0de9dc32"},"id":"d6f3dd61-8c75-48d4-b2a5-79cd0b444ddb","cell_type":"code","execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":"[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /home/repl/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n"}]},{"source":"# Task 1 - Speech to Text: convert the sample audio call, sample_customer_call.wav, to text and store the result in transcribed_text\n\n# Define a recognizer object\nrecognizer = sr.Recognizer()\n\n# Convert the audio file to audio data\ntranscribe_audio_file = sr.AudioFile(\"sample_customer_call.wav\")\nwith transcribe_audio_file as source:\n    transcribe_audio = recognizer.record(source)\n\n# Convert the audio data to text\ntranscribed_text = recognizer.recognize_google(transcribe_audio)\n\n# Review transcribed text\nprint(\"Transcribed text: \", transcribed_text)\n\n# Task 1 - Speech to Text: store few statistics of the audio file such as number of channels, sample width and frame rate\n    \n# Review number of channels and frame rate of the audio file\naudio_segment = AudioSegment.from_file(\"sample_customer_call.wav\")\nnumber_channels = audio_segment.channels\nframe_rate = audio_segment.frame_rate\n\nprint(\"Number of channels: \", number_channels)\nprint(\"Frame rate: \", frame_rate)\n\n# Task 2 - Sentiment Analysis: use vader module from nltk library to determine the sentiment of each text of the customer_call_transcriptions.csv file and store them at a new sentiment_label column using compound score\n\n# Import customer call transcriptions data\ndf = pd.read_csv(\"customer_call_transcriptions.csv\")\n\nsid = SentimentIntensityAnalyzer()\n\n# Analyze sentiment by evaluating compound score generated by Vader SentimentIntensityAnalyzer\ndef find_sentiment(text):\n    scores = sid.polarity_scores(text)\n    compound_score = scores['compound']\n\n    if compound_score >= 0.05:\n        return 'positive'\n    elif compound_score <= -0.05:\n        return 'negative'\n    else:\n        return 'neutral'\n\ndf['sentiment_predicted'] = df.apply(lambda row: find_sentiment(row[\"text\"]), axis = 1)\n\n# Task 2 - Sentiment Analysis: calculate number of texts with positive label that are correctly labeled as positive\ntrue_positive = len(df.loc[(df['sentiment_predicted'] == df['sentiment_label']) &\n                (df['sentiment_label'] == 'positive')])\n\nprint(\"True positives: \", true_positive)\n\n# Task 3 - Named Entity Recognition: find named entities for each text in the df object and store entities in a named_entities column\n\n# Load spaCy small English Language model\nnlp = spacy.load(\"en_core_web_sm\")\n\n# NER using spaCy\ndef extract_entities(text):\n    doc = nlp(text)\n    entities = [ent.text for ent in doc.ents]\n    return entities\n\n# Apply NER to the entire text column\ndf['named_entities'] = df['text'].apply(extract_entities)\n\n# Flatten the list of named entities\nall_entities = [ent for entities in df['named_entities'] for ent in entities]\n\n# Create a DataFrame with the counts\nentities_df = pd.DataFrame(all_entities, columns=['entity'])\nentities_counts = entities_df['entity'].value_counts().reset_index()\nentities_counts.columns = ['entity', 'count']\n\n# Extract most frequent named entity\nmost_freq_ent = entities_counts[\"entity\"].iloc[0]\nprint(\"Most frequent entity: \", most_freq_ent)\n\n# Task 4 - Find most similar text: find the list of customer calls that complained about \"wrong package delivery\" by finding similarity score of each text to the \"wrong package delivery\" string using spaCy small English Language model\n\n# Load spaCy model\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Process the text column\ndf['processed_text'] = df['text'].apply(lambda text: nlp(text))\n\n# Input query\ninput_query = \"wrong package delivery\"\nprocessed_query = nlp(input_query)\n\n# Calculate similarity scores and sort dataframe with respect to similarity scores\ndf['similarity'] = df['processed_text'].apply(lambda text: processed_query.similarity(text))\ndf = df.sort_values(by='similarity', ascending=False)\n\n# Find the most similar text\nmost_similar_text = df[\"text\"].iloc[0]\nprint(\"Most similar text: \", most_similar_text)","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":164,"type":"stream"}},"lastExecutedByKernel":null},"id":"250524c2-1bd3-4ff8-a224-8fa007566c1b","cell_type":"code","execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":"Transcribed text:  hello I'm experiencing an issue with your product I'd like to speak to someone about a replacement\nNumber of channels:  1\nFrame rate:  44100\nTrue positives:  2\nMost frequent entity:  yesterday\nMost similar text:  wrong package delivered\n"}]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}